#!/usr/bin/env python3
"""
è¶…ä¿å®ˆçš„RTX 5090å…¼å®¹è®­ç»ƒè„šæœ¬
ä½¿ç”¨æœ€å°çš„æ¨¡å‹å’Œæœ€å®‰å…¨çš„æ“ä½œ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import os
import numpy as np
import time
import warnings

def safe_gpu_setup():
    """æœ€å®‰å…¨çš„GPUè®¾ç½®"""
    print("ğŸ”§ å®‰å…¨GPUè®¾ç½®...")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        return torch.device('cpu')
    
    try:
        gpu_name = torch.cuda.get_device_name(0)
        print(f"ğŸ® æ£€æµ‹åˆ°GPU: {gpu_name}")
        
        # è®¾ç½®æœ€ä¿å®ˆçš„ç¯å¢ƒ
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        os.environ['TORCH_CUDA_ARCH_LIST'] = '8.0;8.6;8.9;9.0'
        
        # ç¦ç”¨æ‰€æœ‰ä¼˜åŒ–
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.enabled = True
        
        # ç®€å•æµ‹è¯•
        print("ğŸ§ª GPUæµ‹è¯•...")
        test_tensor = torch.tensor([1.0, 2.0], device='cuda')
        result = test_tensor * 2
        print(f"æµ‹è¯•ç»“æœ: {result}")
        
        # æ¸…ç†
        del test_tensor, result
        torch.cuda.empty_cache()
        
        print("âœ… GPUæµ‹è¯•æˆåŠŸ")
        return torch.device('cuda')
        
    except Exception as e:
        print(f"âŒ GPUè®¾ç½®å¤±è´¥: {e}")
        print("ğŸ”„ å›é€€åˆ°CPU")
        return torch.device('cpu')

class TinyDataset(Dataset):
    """è¶…å°æ•°æ®é›†"""
    def __init__(self, size=10):
        self.size = size
        print(f"ğŸ“ åˆ›å»ºå¾®å‹æ•°æ®é›†: {size} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return self.size
    
    def __getitem__(self, idx):
        # ç”Ÿæˆéå¸¸å°çš„å¼ é‡
        lr = torch.randn(1, 8, 8) * 0.1 + 0.5  # å•é€šé“ 8x8
        hr = torch.randn(1, 16, 16) * 0.1 + 0.5  # å•é€šé“ 16x16
        return torch.clamp(lr, 0, 1), torch.clamp(hr, 0, 1)

class TinyModel(nn.Module):
    """è¶…å°æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        
        # æœ€ç®€å•çš„ç½‘ç»œ
        self.net = nn.Sequential(
            nn.Conv2d(1, 8, 3, padding=1),  # 1->8é€šé“
            nn.ReLU(),
            nn.Conv2d(8, 1, 3, padding=1),  # 8->1é€šé“
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # ç®€å•ä¸Šé‡‡æ ·
        x = nn.functional.interpolate(x, scale_factor=2, mode='nearest')
        return self.net(x)

def safe_train_step(model, lr_batch, hr_batch, optimizer, criterion, device):
    """å®‰å…¨çš„è®­ç»ƒæ­¥éª¤"""
    try:
        # ç§»åŠ¨æ•°æ®
        lr_batch = lr_batch.to(device, non_blocking=False)
        hr_batch = hr_batch.to(device, non_blocking=False)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        output = model(lr_batch)
        loss = criterion(output, hr_batch)
        
        # æ£€æŸ¥æŸå¤±
        if torch.isnan(loss) or torch.isinf(loss):
            print("âš ï¸ æ£€æµ‹åˆ°NaN/InfæŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            return None
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        return loss.item()
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥: {e}")
        return None

def main():
    print("ğŸš€ è¶…ä¿å®ˆRTX 5090å…¼å®¹è®­ç»ƒ")
    print("=" * 40)
    
    # å¿½ç•¥è­¦å‘Š
    warnings.filterwarnings("ignore")
    
    # è®¾ç½®è®¾å¤‡
    device = safe_gpu_setup()
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    if device.type == 'cuda':
        memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPUå†…å­˜: {memory_gb:.1f} GB")
    
    # è¶…ä¿å®ˆé…ç½®
    config = {
        'epochs': 2,
        'batch_size': 1,  # å•ä¸ªæ ·æœ¬
        'learning_rate': 1e-4,
        'num_samples': 5  # åªæœ‰5ä¸ªæ ·æœ¬
    }
    
    print("\nè®­ç»ƒé…ç½®:")
    for k, v in config.items():
        print(f"  {k}: {v}")
    
    # åˆ›å»ºæ•°æ®
    dataset = TinyDataset(config['num_samples'])
    dataloader = DataLoader(
        dataset, 
        batch_size=config['batch_size'], 
        shuffle=False,  # ä¸æ‰“ä¹±
        num_workers=0,
        pin_memory=False
    )
    
    print(f"æ‰¹æ¬¡æ•°: {len(dataloader)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ§  åˆ›å»ºè¶…å°æ¨¡å‹...")
    model = TinyModel().to(device)
    
    # å‚æ•°ç»Ÿè®¡
    params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°: {params:,}")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±
    optimizer = optim.SGD(model.parameters(), lr=config['learning_rate'])  # ä½¿ç”¨SGD
    criterion = nn.MSELoss()
    
    print(f"\nğŸ¯ å¼€å§‹è¶…ä¿å®ˆè®­ç»ƒ...")
    print("=" * 25)
    
    history = []
    
    try:
        for epoch in range(1, config['epochs'] + 1):
            print(f"\nEpoch {epoch}:")
            model.train()
            
            epoch_losses = []
            
            for batch_idx, (lr_batch, hr_batch) in enumerate(dataloader):
                print(f"  å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(dataloader)}...")
                
                # å®‰å…¨è®­ç»ƒæ­¥éª¤
                loss = safe_train_step(model, lr_batch, hr_batch, optimizer, criterion, device)
                
                if loss is not None:
                    epoch_losses.append(loss)
                    print(f"    æŸå¤±: {loss:.6f}")
                else:
                    print("    æ‰¹æ¬¡è·³è¿‡")
                
                # é¢‘ç¹æ¸…ç†GPUå†…å­˜
                if device.type == 'cuda':
                    torch.cuda.empty_cache()
                
                # å°å»¶è¿Ÿ
                time.sleep(0.1)
            
            # è®¡ç®—å¹³å‡æŸå¤±
            if epoch_losses:
                avg_loss = sum(epoch_losses) / len(epoch_losses)
                history.append(avg_loss)
                print(f"  Epoch {epoch} å¹³å‡æŸå¤±: {avg_loss:.6f}")
            else:
                print(f"  Epoch {epoch} æ²¡æœ‰æœ‰æ•ˆæŸå¤±")
        
        print("\nâœ… è®­ç»ƒå®Œæˆ!")
        
        if history:
            print(f"æœ€ç»ˆæŸå¤±: {history[-1]:.6f}")
            
            # ä¿å­˜æ¨¡å‹
            model_path = "tiny_rtx5090_model.pth"
            torch.save({
                'model_state_dict': model.state_dict(),
                'config': config,
                'history': history
            }, model_path)
            print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
        
        # ç®€å•æµ‹è¯•
        print("\nğŸ§ª æ¨¡å‹æµ‹è¯•...")
        model.eval()
        with torch.no_grad():
            test_input = torch.randn(1, 1, 8, 8).to(device)
            test_output = model(test_input)
            print(f"è¾“å…¥: {test_input.shape} -> è¾“å‡º: {test_output.shape}")
            print("âœ… æµ‹è¯•é€šè¿‡!")
        
        # æœ€ç»ˆæ¸…ç†
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
        if device.type == 'cuda':
            torch.cuda.empty_cache()
    
    return model, history

if __name__ == "__main__":
    model, history = main()