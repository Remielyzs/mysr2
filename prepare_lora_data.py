#!/usr/bin/env python3
"""
LoRAå¾®è°ƒæ•°æ®å‡†å¤‡è„šæœ¬
ç”¨äºå‡†å¤‡å’Œé¢„å¤„ç†å›¾åƒæ•°æ®
"""

import os
import shutil
from pathlib import Path
import argparse
from PIL import Image
import torch
import torchvision.transforms as transforms
from typing import List, Tuple
import json

def create_data_structure():
    """åˆ›å»ºæ•°æ®ç›®å½•ç»“æ„"""
    data_dir = Path("./data")
    
    # åˆ›å»ºç›®å½•
    dirs_to_create = [
        data_dir / "images" / "train",
        data_dir / "images" / "val",
        data_dir / "processed" / "lr",
        data_dir / "processed" / "hr",
    ]
    
    for dir_path in dirs_to_create:
        dir_path.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºç›®å½•: {dir_path}")
    
    return data_dir

def find_images(directory: Path) -> List[Path]:
    """æŸ¥æ‰¾ç›®å½•ä¸­çš„å›¾åƒæ–‡ä»¶"""
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
    image_files = []
    
    if directory.exists():
        for file_path in directory.rglob('*'):
            if file_path.suffix.lower() in image_extensions:
                image_files.append(file_path)
    
    return image_files

def process_image_for_sr(image_path: Path, lr_size: int = 64, hr_size: int = 256) -> Tuple[Image.Image, Image.Image]:
    """å¤„ç†å›¾åƒç”ŸæˆLRå’ŒHRå¯¹"""
    try:
        # åŠ è½½å›¾åƒ
        image = Image.open(image_path).convert('RGB')
        
        # åˆ›å»ºHRç‰ˆæœ¬ï¼ˆé«˜åˆ†è¾¨ç‡ï¼‰
        hr_transform = transforms.Compose([
            transforms.Resize((hr_size, hr_size), Image.LANCZOS),
        ])
        hr_image = hr_transform(image)
        
        # åˆ›å»ºLRç‰ˆæœ¬ï¼ˆä½åˆ†è¾¨ç‡ï¼‰
        lr_transform = transforms.Compose([
            transforms.Resize((lr_size, lr_size), Image.LANCZOS),
        ])
        lr_image = lr_transform(image)
        
        return lr_image, hr_image
        
    except Exception as e:
        print(f"âŒ å¤„ç†å›¾åƒå¤±è´¥ {image_path}: {e}")
        return None, None

def prepare_dataset(source_dir: str, output_dir: str, lr_size: int = 64, hr_size: int = 256, max_images: int = None):
    """å‡†å¤‡æ•°æ®é›†"""
    source_path = Path(source_dir)
    output_path = Path(output_dir)
    
    print(f"ğŸ” åœ¨ {source_path} ä¸­æŸ¥æ‰¾å›¾åƒ...")
    image_files = find_images(source_path)
    
    if not image_files:
        print(f"âš ï¸ åœ¨ {source_path} ä¸­æœªæ‰¾åˆ°å›¾åƒæ–‡ä»¶")
        return False
    
    print(f"ğŸ“¸ æ‰¾åˆ° {len(image_files)} ä¸ªå›¾åƒæ–‡ä»¶")
    
    if max_images:
        image_files = image_files[:max_images]
        print(f"ğŸ“Š é™åˆ¶å¤„ç† {len(image_files)} ä¸ªå›¾åƒ")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    lr_dir = output_path / "lr"
    hr_dir = output_path / "hr"
    lr_dir.mkdir(parents=True, exist_ok=True)
    hr_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤„ç†å›¾åƒ
    processed_count = 0
    failed_count = 0
    
    for i, image_path in enumerate(image_files):
        print(f"å¤„ç† [{i+1}/{len(image_files)}]: {image_path.name}")
        
        lr_image, hr_image = process_image_for_sr(image_path, lr_size, hr_size)
        
        if lr_image and hr_image:
            # ä¿å­˜LRå’ŒHRå›¾åƒ
            base_name = image_path.stem
            lr_path = lr_dir / f"{base_name}_lr.png"
            hr_path = hr_dir / f"{base_name}_hr.png"
            
            lr_image.save(lr_path)
            hr_image.save(hr_path)
            
            processed_count += 1
        else:
            failed_count += 1
    
    print(f"\nâœ… æ•°æ®å‡†å¤‡å®Œæˆ!")
    print(f"ğŸ“Š æˆåŠŸå¤„ç†: {processed_count} ä¸ªå›¾åƒ")
    print(f"âŒ å¤±è´¥: {failed_count} ä¸ªå›¾åƒ")
    print(f"ğŸ“ LRå›¾åƒä¿å­˜åˆ°: {lr_dir}")
    print(f"ğŸ“ HRå›¾åƒä¿å­˜åˆ°: {hr_dir}")
    
    # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
    dataset_info = {
        'total_images': len(image_files),
        'processed_images': processed_count,
        'failed_images': failed_count,
        'lr_size': lr_size,
        'hr_size': hr_size,
        'lr_dir': str(lr_dir),
        'hr_dir': str(hr_dir)
    }
    
    info_path = output_path / "dataset_info.json"
    with open(info_path, 'w') as f:
        json.dump(dataset_info, f, indent=2)
    
    print(f"ğŸ“‹ æ•°æ®é›†ä¿¡æ¯ä¿å­˜åˆ°: {info_path}")
    return True

def create_sample_images(output_dir: str, num_samples: int = 50):
    """åˆ›å»ºç¤ºä¾‹å›¾åƒç”¨äºæµ‹è¯•"""
    output_path = Path(output_dir)
    sample_dir = output_path / "sample_images"
    sample_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ¨ åˆ›å»º {num_samples} ä¸ªç¤ºä¾‹å›¾åƒ...")
    
    # åˆ›å»ºä¸åŒç±»å‹çš„ç¤ºä¾‹å›¾åƒ
    for i in range(num_samples):
        # åˆ›å»ºéšæœºå½©è‰²å›¾åƒ
        image = Image.new('RGB', (512, 512))
        pixels = []
        
        for y in range(512):
            for x in range(512):
                # åˆ›å»ºæ¸å˜æ•ˆæœ
                r = int(255 * (x / 512))
                g = int(255 * (y / 512))
                b = int(255 * ((x + y) / 1024))
                
                # æ·»åŠ ä¸€äº›éšæœºæ€§
                import random
                r = max(0, min(255, r + random.randint(-50, 50)))
                g = max(0, min(255, g + random.randint(-50, 50)))
                b = max(0, min(255, b + random.randint(-50, 50)))
                
                pixels.append((r, g, b))
        
        image.putdata(pixels)
        
        # ä¿å­˜å›¾åƒ
        image_path = sample_dir / f"sample_{i:03d}.png"
        image.save(image_path)
    
    print(f"âœ… ç¤ºä¾‹å›¾åƒåˆ›å»ºå®Œæˆï¼Œä¿å­˜åˆ°: {sample_dir}")
    return sample_dir

def main():
    parser = argparse.ArgumentParser(description="LoRAå¾®è°ƒæ•°æ®å‡†å¤‡")
    parser.add_argument("--source", type=str, help="æºå›¾åƒç›®å½•")
    parser.add_argument("--output", type=str, default="./data/processed", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--lr-size", type=int, default=64, help="LRå›¾åƒå°ºå¯¸")
    parser.add_argument("--hr-size", type=int, default=256, help="HRå›¾åƒå°ºå¯¸")
    parser.add_argument("--max-images", type=int, help="æœ€å¤§å¤„ç†å›¾åƒæ•°")
    parser.add_argument("--create-samples", action="store_true", help="åˆ›å»ºç¤ºä¾‹å›¾åƒ")
    parser.add_argument("--num-samples", type=int, default=50, help="ç¤ºä¾‹å›¾åƒæ•°é‡")
    
    args = parser.parse_args()
    
    print("ğŸš€ LoRAå¾®è°ƒæ•°æ®å‡†å¤‡")
    print("=" * 40)
    
    # åˆ›å»ºæ•°æ®ç›®å½•ç»“æ„
    data_dir = create_data_structure()
    
    if args.create_samples:
        # åˆ›å»ºç¤ºä¾‹å›¾åƒ
        sample_dir = create_sample_images(data_dir, args.num_samples)
        
        # ä½¿ç”¨ç¤ºä¾‹å›¾åƒä½œä¸ºæº
        if not args.source:
            args.source = str(sample_dir)
            print(f"ğŸ“ ä½¿ç”¨ç¤ºä¾‹å›¾åƒä½œä¸ºæº: {args.source}")
    
    if args.source:
        # å‡†å¤‡æ•°æ®é›†
        success = prepare_dataset(
            source_dir=args.source,
            output_dir=args.output,
            lr_size=args.lr_size,
            hr_size=args.hr_size,
            max_images=args.max_images
        )
        
        if success:
            print(f"\nğŸ¯ æ•°æ®å‡†å¤‡å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ")
            print(f"è¿è¡Œå‘½ä»¤: python train_lora_stable_diffusion.py")
        else:
            print(f"\nâŒ æ•°æ®å‡†å¤‡å¤±è´¥")
    else:
        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print(f"  åˆ›å»ºç¤ºä¾‹æ•°æ®: python prepare_lora_data.py --create-samples")
        print(f"  å¤„ç†ç°æœ‰å›¾åƒ: python prepare_lora_data.py --source /path/to/images")
        print(f"  å®Œæ•´ç¤ºä¾‹: python prepare_lora_data.py --source ./images --lr-size 64 --hr-size 256 --max-images 100")

if __name__ == "__main__":
    main()