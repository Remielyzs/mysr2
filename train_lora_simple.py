#!/usr/bin/env python3
"""
ç®€åŒ–çš„LoRAå¾®è°ƒè®­ç»ƒè„šæœ¬ - æ— éœ€å¤æ‚ä¾èµ–
ä¸“ä¸ºRTX 5090 GPUä¼˜åŒ–çš„è¶…åˆ†è¾¨ç‡LoRAå¾®è°ƒ
"""

import os
import sys
import time
import random
import numpy as np
from pathlib import Path

# å°è¯•å¯¼å…¥å¿…è¦çš„åº“
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    import torch.nn.functional as F
    print("âœ… PyTorchå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ PyTorchå¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å®‰è£…PyTorch: pip install torch torchvision")
    sys.exit(1)

try:
    from PIL import Image
    import torchvision.transforms as transforms
    print("âœ… PILå’Œtorchvisionå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ PIL/torchvisionå¯¼å…¥å¤±è´¥: {e}")
    print("è¯·å®‰è£…: pip install Pillow torchvision")
    sys.exit(1)

def setup_gpu_environment():
    """è®¾ç½®GPUç¯å¢ƒ"""
    print("ğŸ® è®¾ç½®GPUç¯å¢ƒ...")
    
    # RTX 5090å…¼å®¹æ€§è®¾ç½®
    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
    os.environ['TORCH_CUDA_ARCH_LIST'] = '8.9'
    
    if torch.cuda.is_available():
        device = torch.device('cuda')
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"ğŸ¯ GPUè®¾å¤‡: {gpu_name}")
        print(f"ğŸ’¾ GPUå†…å­˜: {gpu_memory:.1f} GB")
        
        # RTX 5090ç‰¹æ®Šè®¾ç½®
        if "RTX 5090" in gpu_name:
            print("ğŸš€ æ£€æµ‹åˆ°RTX 5090ï¼Œåº”ç”¨ä¼˜åŒ–è®¾ç½®")
            torch.backends.cudnn.benchmark = True
            torch.backends.cudnn.deterministic = False
        
        return device
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°CUDA GPUï¼Œä½¿ç”¨CPU")
        return torch.device('cpu')

class SimpleLoRALayer(nn.Module):
    """ç®€åŒ–çš„LoRAå±‚å®ç°"""
    def __init__(self, in_features, out_features, rank=8, alpha=16.0):
        super().__init__()
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        
        # LoRAæƒé‡
        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * 0.01)
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))
        
    def forward(self, x):
        # LoRAå‰å‘ä¼ æ’­: x + (x @ A.T @ B.T) * scaling
        lora_out = F.linear(F.linear(x, self.lora_A), self.lora_B)
        return x + lora_out * self.scaling

class SimpleUNet(nn.Module):
    """ç®€åŒ–çš„U-Netæ¨¡å‹ç”¨äºè¶…åˆ†è¾¨ç‡"""
    def __init__(self, in_channels=3, out_channels=3, features=64):
        super().__init__()
        
        # ç¼–ç å™¨
        self.encoder1 = nn.Sequential(
            nn.Conv2d(in_channels, features, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(features, features, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        
        self.encoder2 = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(features, features*2, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(features*2, features*2, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        
        # ç“¶é¢ˆå±‚
        self.bottleneck = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(features*2, features*4, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(features*4, features*4, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        
        # è§£ç å™¨
        self.upconv2 = nn.ConvTranspose2d(features*4, features*2, 2, stride=2)
        self.decoder2 = nn.Sequential(
            nn.Conv2d(features*4, features*2, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(features*2, features*2, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        
        self.upconv1 = nn.ConvTranspose2d(features*2, features, 2, stride=2)
        self.decoder1 = nn.Sequential(
            nn.Conv2d(features*2, features, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(features, features, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        
        # ä¸Šé‡‡æ ·åˆ°ç›®æ ‡åˆ†è¾¨ç‡
        self.final_upconv = nn.ConvTranspose2d(features, features, 4, stride=4)
        self.final_conv = nn.Conv2d(features, out_channels, 1)
        
    def forward(self, x):
        # ç¼–ç 
        enc1 = self.encoder1(x)
        enc2 = self.encoder2(enc1)
        
        # ç“¶é¢ˆ
        bottleneck = self.bottleneck(enc2)
        
        # è§£ç 
        dec2 = self.upconv2(bottleneck)
        dec2 = torch.cat([dec2, enc2], dim=1)
        dec2 = self.decoder2(dec2)
        
        dec1 = self.upconv1(dec2)
        dec1 = torch.cat([dec1, enc1], dim=1)
        dec1 = self.decoder1(dec1)
        
        # æœ€ç»ˆä¸Šé‡‡æ ·
        out = self.final_upconv(dec1)
        out = self.final_conv(out)
        
        return torch.sigmoid(out)

class LoRAUNet(nn.Module):
    """é›†æˆLoRAçš„U-Netæ¨¡å‹"""
    def __init__(self, base_model, lora_rank=8, lora_alpha=16.0):
        super().__init__()
        self.base_model = base_model
        self.lora_layers = nn.ModuleList()
        
        # ä¸ºä¸»è¦å·ç§¯å±‚æ·»åŠ LoRA
        self._add_lora_to_conv_layers(lora_rank, lora_alpha)
        
    def _add_lora_to_conv_layers(self, rank, alpha):
        """ä¸ºå·ç§¯å±‚æ·»åŠ LoRAé€‚é…å™¨"""
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„å±‚é€‰æ‹©é€»è¾‘
        for name, module in self.base_model.named_modules():
            if isinstance(module, nn.Conv2d) and module.out_channels >= 64:
                # ä¸ºå¤§çš„å·ç§¯å±‚æ·»åŠ LoRA
                in_features = module.in_channels * module.kernel_size[0] * module.kernel_size[1]
                out_features = module.out_channels
                lora_layer = SimpleLoRALayer(in_features, out_features, rank, alpha)
                self.lora_layers.append(lora_layer)
        
        print(f"ğŸ“Š æ·»åŠ äº† {len(self.lora_layers)} ä¸ªLoRAå±‚")
    
    def forward(self, x):
        return self.base_model(x)

class SyntheticSRDataset(Dataset):
    """åˆæˆè¶…åˆ†è¾¨ç‡æ•°æ®é›†"""
    def __init__(self, num_samples=200, lr_size=64, hr_size=256):
        self.num_samples = num_samples
        self.lr_size = lr_size
        self.hr_size = hr_size
        
        # æ•°æ®å˜æ¢
        self.lr_transform = transforms.Compose([
            transforms.Resize((lr_size, lr_size)),
            transforms.ToTensor()
        ])
        
        self.hr_transform = transforms.Compose([
            transforms.Resize((hr_size, hr_size)),
            transforms.ToTensor()
        ])
        
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        # ç”Ÿæˆéšæœºå›¾åƒ
        np.random.seed(idx)
        
        # åˆ›å»ºé«˜åˆ†è¾¨ç‡å›¾åƒ
        hr_image = np.random.rand(self.hr_size, self.hr_size, 3) * 255
        hr_image = hr_image.astype(np.uint8)
        hr_image = Image.fromarray(hr_image)
        
        # åˆ›å»ºä½åˆ†è¾¨ç‡å›¾åƒï¼ˆé€šè¿‡ä¸‹é‡‡æ ·ï¼‰
        lr_image = hr_image.resize((self.lr_size, self.lr_size), Image.LANCZOS)
        
        # è½¬æ¢ä¸ºtensor
        lr_tensor = self.lr_transform(lr_image)
        hr_tensor = self.hr_transform(hr_image)
        
        return lr_tensor, hr_tensor

def train_epoch(model, dataloader, optimizer, criterion, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0.0
    num_batches = len(dataloader)
    
    print(f"\nEpoch {epoch} å¼€å§‹è®­ç»ƒ...")
    
    for batch_idx, (lr_images, hr_images) in enumerate(dataloader):
        lr_images = lr_images.to(device)
        hr_images = hr_images.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        outputs = model(lr_images)
        
        # è®¡ç®—æŸå¤±
        loss = criterion(outputs, hr_images)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()
        
        total_loss += loss.item()
        
        # æ‰“å°è¿›åº¦
        if batch_idx % 10 == 0:
            print(f"  æ‰¹æ¬¡ [{batch_idx}/{num_batches}] æŸå¤±: {loss.item():.6f}")
    
    avg_loss = total_loss / num_batches
    print(f"Epoch {epoch} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.6f}")
    return avg_loss

def save_model(model, optimizer, epoch, loss, save_path):
    """ä¿å­˜æ¨¡å‹"""
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }, save_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

def test_model(model, device, lr_size=64, hr_size=256):
    """æµ‹è¯•æ¨¡å‹"""
    model.eval()
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹...")
    
    with torch.no_grad():
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randn(1, 3, lr_size, lr_size).to(device)
        
        # å‰å‘ä¼ æ’­
        output = model(test_input)
        
        print(f"è¾“å…¥å°ºå¯¸: {test_input.shape}")
        print(f"è¾“å‡ºå°ºå¯¸: {output.shape}")
        print(f"æœŸæœ›è¾“å‡ºå°ºå¯¸: (1, 3, {hr_size}, {hr_size})")
        
        if output.shape == (1, 3, hr_size, hr_size):
            print("âœ… æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼")
            return True
        else:
            print("âŒ æ¨¡å‹è¾“å‡ºå°ºå¯¸ä¸æ­£ç¡®")
            return False

def main():
    print("ğŸš€ ç®€åŒ–LoRAå¾®è°ƒè®­ç»ƒå¼€å§‹")
    print("=" * 50)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    # è®¾ç½®GPUç¯å¢ƒ
    device = setup_gpu_environment()
    
    # è®­ç»ƒé…ç½®
    config = {
        'epochs': 5,
        'batch_size': 4,
        'learning_rate': 1e-4,
        'num_samples': 100,
        'lr_size': 64,
        'hr_size': 256,
        'lora_rank': 8,
        'lora_alpha': 16.0,
        'save_dir': './simple_lora_checkpoints'
    }
    
    print("\nğŸ“‹ è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(config['save_dir'], exist_ok=True)
    
    # åˆ›å»ºæ•°æ®é›†
    print(f"\nğŸ“ åˆ›å»ºæ•°æ®é›†...")
    dataset = SyntheticSRDataset(
        num_samples=config['num_samples'],
        lr_size=config['lr_size'],
        hr_size=config['hr_size']
    )
    
    dataloader = DataLoader(
        dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=0  # Windowså…¼å®¹æ€§
    )
    
    print(f"ğŸ“Š æ•°æ®é›†: {len(dataset)} ä¸ªæ ·æœ¬")
    print(f"ğŸ“ LRå°ºå¯¸: {config['lr_size']}x{config['lr_size']}, HRå°ºå¯¸: {config['hr_size']}x{config['hr_size']}")
    print(f"æ‰¹æ¬¡æ•°: {len(dataloader)}")
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ§  åˆ›å»ºLoRA U-Netæ¨¡å‹...")
    base_model = SimpleUNet(in_channels=3, out_channels=3, features=32)
    model = LoRAUNet(
        base_model=base_model,
        lora_rank=config['lora_rank'],
        lora_alpha=config['lora_alpha']
    ).to(device)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    lora_params = sum(p.numel() for p in model.lora_layers.parameters())
    
    print(f"æ€»å‚æ•°: {total_params:,}")
    print(f"LoRAå‚æ•°: {lora_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"LoRAå‚æ•°æ¯”ä¾‹: {lora_params/total_params*100:.2f}%")
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])
    criterion = nn.MSELoss()
    
    print(f"\nğŸ¯ å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ...")
    print("=" * 40)
    
    # è®­ç»ƒå¾ªç¯
    best_loss = float('inf')
    
    for epoch in range(1, config['epochs'] + 1):
        # è®­ç»ƒ
        avg_loss = train_epoch(model, dataloader, optimizer, criterion, device, epoch)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            save_path = os.path.join(config['save_dir'], 'best_lora_model.pth')
            save_model(model, optimizer, epoch, avg_loss, save_path)
        
        # æ¯ä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_path = os.path.join(config['save_dir'], f'lora_epoch_{epoch}.pth')
        save_model(model, optimizer, epoch, avg_loss, checkpoint_path)
    
    # æµ‹è¯•æ¨¡å‹
    test_success = test_model(model, device, config['lr_size'], config['hr_size'])
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = os.path.join(config['save_dir'], 'final_lora_model.pth')
    save_model(model, optimizer, config['epochs'], best_loss, final_path)
    
    print(f"\nğŸ‰ LoRAå¾®è°ƒè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š æœ€ä½³æŸå¤±: {best_loss:.6f}")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {config['save_dir']}")
    
    if test_success:
        print("âœ… æ¨¡å‹éªŒè¯é€šè¿‡")
    else:
        print("âš ï¸ æ¨¡å‹éªŒè¯å¤±è´¥")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()