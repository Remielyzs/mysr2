#!/usr/bin/env python3
"""
ç®€åŒ–ä½†GPUå…¼å®¹çš„æ‰©æ•£æ¨¡å‹è®­ç»ƒè„šæœ¬
ä¸“é—¨é’ˆå¯¹RTX 5090ä¼˜åŒ–ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„æ–¹æ³•
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import os
import numpy as np
import time
import warnings

def setup_rtx5090_compatibility():
    """ä¸“é—¨ä¸ºRTX 5090è®¾ç½®å…¼å®¹æ€§"""
    print("ğŸ”§ RTX 5090å…¼å®¹æ€§è®¾ç½®...")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return torch.device('cpu')
    
    gpu_name = torch.cuda.get_device_name(0)
    print(f"ğŸ® GPU: {gpu_name}")
    
    # RTX 5090ç‰¹æ®Šå¤„ç†
    if "RTX 5090" in gpu_name:
        print("âš ï¸ åº”ç”¨RTX 5090å…¼å®¹æ€§ä¿®æ­£...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
        os.environ['TORCH_CUDA_ARCH_LIST'] = '8.0;8.6;8.9;9.0'
        
        # å¯ç”¨å…¼å®¹æ¨¡å¼
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = False  # æ›´ä¿å®ˆçš„è®¾ç½®
        torch.backends.cudnn.deterministic = True
        
        try:
            # ç®€å•æµ‹è¯•
            test = torch.randn(2, 2, device='cuda')
            result = test @ test
            del test, result
            torch.cuda.empty_cache()
            
            print("âœ… RTX 5090å…¼å®¹æ€§æµ‹è¯•é€šè¿‡")
            return torch.device('cuda')
            
        except Exception as e:
            print(f"âŒ GPUæµ‹è¯•å¤±è´¥: {e}")
            return torch.device('cpu')
    
    # å…¶ä»–GPU
    try:
        test = torch.randn(2, 2, device='cuda')
        del test
        return torch.device('cuda')
    except:
        return torch.device('cpu')

class SimpleDataset(Dataset):
    """æç®€æ•°æ®é›†"""
    def __init__(self, size=50):
        self.size = size
        print(f"ğŸ“ åˆ›å»ºç®€å•æ•°æ®é›†: {size} ä¸ªæ ·æœ¬")
    
    def __len__(self):
        return self.size
    
    def __getitem__(self, idx):
        # ç”Ÿæˆç®€å•çš„LRå’ŒHRå¯¹
        lr = torch.randn(3, 16, 16) * 0.5 + 0.5  # 16x16 LR
        hr = torch.randn(3, 64, 64) * 0.5 + 0.5  # 64x64 HR
        return torch.clamp(lr, 0, 1), torch.clamp(hr, 0, 1)

class MinimalUNet(nn.Module):
    """æœ€å°åŒ–çš„U-Netï¼Œä¸“ä¸ºGPUå…¼å®¹æ€§ä¼˜åŒ–"""
    def __init__(self):
        super().__init__()
        
        # æç®€ç¼–ç å™¨
        self.enc = nn.Sequential(
            nn.Conv2d(3, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(2)
        )
        
        # æç®€è§£ç å™¨
        self.dec = nn.Sequential(
            nn.ConvTranspose2d(64, 32, 2, stride=2),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 3, 3, padding=1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # ä¸Šé‡‡æ ·è¾“å…¥
        x = nn.functional.interpolate(x, scale_factor=4, mode='nearest')
        
        # ç¼–ç -è§£ç 
        encoded = self.enc(x)
        decoded = self.dec(encoded)
        
        return decoded

def train_simple_epoch(model, dataloader, optimizer, criterion, device, epoch):
    """ç®€åŒ–çš„è®­ç»ƒå¾ªç¯"""
    model.train()
    total_loss = 0
    count = 0
    
    print(f"Epoch {epoch} å¼€å§‹...")
    
    for batch_idx, (lr, hr) in enumerate(dataloader):
        try:
            # ç§»åŠ¨åˆ°è®¾å¤‡
            lr = lr.to(device)
            hr = hr.to(device)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            output = model(lr)
            loss = criterion(output, hr)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            count += 1
            
            # æ¯5ä¸ªæ‰¹æ¬¡æ‰“å°ä¸€æ¬¡
            if batch_idx % 5 == 0:
                print(f"  Batch {batch_idx}, Loss: {loss.item():.6f}")
                
            # æ¸…ç†GPUå†…å­˜
            if device.type == 'cuda' and batch_idx % 10 == 0:
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {batch_idx} å¤±è´¥: {e}")
            continue
    
    avg_loss = total_loss / count if count > 0 else 0
    print(f"Epoch {epoch} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.6f}")
    return avg_loss

def main():
    print("ğŸš€ RTX 5090å…¼å®¹çš„ç®€åŒ–æ‰©æ•£æ¨¡å‹è®­ç»ƒ")
    print("=" * 50)
    
    # å¿½ç•¥è­¦å‘Š
    warnings.filterwarnings("ignore")
    
    # è®¾ç½®è®¾å¤‡
    device = setup_rtx5090_compatibility()
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    if device.type == 'cuda':
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # ç®€åŒ–é…ç½®
    config = {
        'epochs': 3,
        'batch_size': 2,  # å°æ‰¹æ¬¡å¤§å°
        'learning_rate': 1e-3,
        'num_samples': 30
    }
    
    print("è®­ç»ƒé…ç½®:")
    for k, v in config.items():
        print(f"  {k}: {v}")
    
    # åˆ›å»ºæ•°æ®
    dataset = SimpleDataset(config['num_samples'])
    dataloader = DataLoader(
        dataset, 
        batch_size=config['batch_size'], 
        shuffle=True,
        num_workers=0  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    print(f"æ‰¹æ¬¡æ•°: {len(dataloader)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ§  åˆ›å»ºæ¨¡å‹...")
    model = MinimalUNet().to(device)
    
    # å‚æ•°ç»Ÿè®¡
    params = sum(p.numel() for p in model.parameters())
    print(f"æ¨¡å‹å‚æ•°: {params:,}")
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])
    criterion = nn.MSELoss()
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print("=" * 30)
    
    history = []
    
    try:
        for epoch in range(1, config['epochs'] + 1):
            start_time = time.time()
            
            # è®­ç»ƒ
            loss = train_simple_epoch(model, dataloader, optimizer, criterion, device, epoch)
            history.append(loss)
            
            epoch_time = time.time() - start_time
            print(f"Epoch {epoch} ç”¨æ—¶: {epoch_time:.2f}ç§’")
            print("-" * 20)
            
            # GPUå†…å­˜æ¸…ç†
            if device.type == 'cuda':
                torch.cuda.empty_cache()
        
        print("\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ç»ˆæŸå¤±: {history[-1]:.6f}")
        
        # ä¿å­˜æ¨¡å‹
        model_path = "rtx5090_compatible_model.pth"
        torch.save({
            'model_state_dict': model.state_dict(),
            'config': config,
            'history': history
        }, model_path)
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åˆ°: {model_path}")
        
        # æµ‹è¯•
        print("\nğŸ§ª æ¨¡å‹æµ‹è¯•...")
        model.eval()
        with torch.no_grad():
            test_input = torch.randn(1, 3, 16, 16).to(device)
            test_output = model(test_input)
            print(f"è¾“å…¥: {test_input.shape} -> è¾“å‡º: {test_output.shape}")
            print("âœ… æµ‹è¯•é€šè¿‡!")
        
        # æœ€ç»ˆæ¸…ç†
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")
            
    except Exception as e:
        print(f"âŒ è®­ç»ƒé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        
        if device.type == 'cuda':
            torch.cuda.empty_cache()
    
    return model, history

if __name__ == "__main__":
    model, history = main()