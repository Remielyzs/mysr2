#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä»£ç è´¨é‡å’Œå¯ç»´æŠ¤æ€§æ”¹è¿›

è¿™ä¸ªè„šæœ¬ç”¨äºéªŒè¯æ‰€æœ‰æ”¹è¿›æ˜¯å¦æ­£å¸¸å·¥ä½œï¼ŒåŒ…æ‹¬ï¼š
1. ConfigManager é…ç½®ç®¡ç†
2. TrainingLogger æ—¥å¿—ç³»ç»Ÿ
3. BaseTrainer å’Œ DiffusionTrainer çš„æ”¹è¿›
4. é”™è¯¯å¤„ç†å’Œç›®å½•ç®¡ç†
"""

import os
import sys
import torch
import tempfile
import shutil
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from utils.config_manager import ConfigManager
from utils.logger import TrainingLogger
from trainers.diffusion_trainer import DiffusionTrainer

def test_config_manager():
    """æµ‹è¯•é…ç½®ç®¡ç†å™¨"""
    print("\n=== æµ‹è¯• ConfigManager ===")
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    # æä¾›å¿…éœ€çš„é…ç½®é¡¹
    required_config = {
        'train_lr_dir': '/tmp/train/lr',
        'train_hr_dir': '/tmp/train/hr',
        'val_lr_dir': '/tmp/val/lr',
        'val_hr_dir': '/tmp/val/hr'
    }
    config = ConfigManager(required_config)
    
    # æµ‹è¯•é»˜è®¤å€¼
    assert config.get('batch_size') == 8, "é»˜è®¤batch_sizeåº”è¯¥æ˜¯8"
    assert config.get('learning_rate') == 1e-4, "é»˜è®¤learning_rateåº”è¯¥æ˜¯1e-4"
    
    # æµ‹è¯•è®¾ç½®å’Œè·å–
    config['custom_param'] = 'test_value'  # ä½¿ç”¨å­—å…¸å¼è®¿é—®
    assert config.get('custom_param') == 'test_value', "è‡ªå®šä¹‰å‚æ•°è®¾ç½®å¤±è´¥"
    
    # æµ‹è¯•éªŒè¯ï¼ˆé€šè¿‡update_configæ–¹æ³•ï¼‰
    try:
        test_config = ConfigManager(required_config.copy())
        test_config.update_config({'batch_size': -1})  # åº”è¯¥å¤±è´¥
        assert False, "è´Ÿæ•°batch_sizeåº”è¯¥è¢«æ‹’ç»"
    except ValueError:
        pass  # é¢„æœŸçš„é”™è¯¯
    
    # æµ‹è¯•åºåˆ—åŒ–ï¼ˆé€šè¿‡ç›´æ¥è®¿é—®_configï¼‰
    config_dict = config._config.copy()
    assert isinstance(config_dict, dict), "_configåº”è¯¥æ˜¯å­—å…¸"
    
    # æµ‹è¯•ä»å­—å…¸åˆ›å»ºï¼ˆç¡®ä¿ä½¿ç”¨æœ‰æ•ˆçš„é…ç½®ï¼‰
    valid_config_dict = required_config.copy()
    valid_config_dict.update({'batch_size': 8, 'learning_rate': 1e-4})
    new_config = ConfigManager(valid_config_dict)
    assert new_config.get('batch_size') == 8, "ä»å­—å…¸åˆ›å»ºçš„é…ç½®åº”è¯¥ç›¸åŒ"
    
    print("âœ“ ConfigManager æµ‹è¯•é€šè¿‡")

def test_training_logger():
    """æµ‹è¯•è®­ç»ƒæ—¥å¿—å™¨"""
    print("\n=== æµ‹è¯• TrainingLogger ===")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        logger = TrainingLogger(
            name="test_logger",
            log_dir=temp_dir,
            level="INFO"
        )
        
        # æµ‹è¯•åŸºæœ¬æ—¥å¿—
        logger.info("æµ‹è¯•ä¿¡æ¯æ—¥å¿—")
        logger.warning("æµ‹è¯•è­¦å‘Šæ—¥å¿—")
        logger.error("æµ‹è¯•é”™è¯¯æ—¥å¿—")
        
        # æµ‹è¯•epochæ—¥å¿—
        logger.log_epoch_start(0, 10)
        logger.log_epoch_end(0, 0.5, 0.4)
        
        # æµ‹è¯•æ¨¡å‹æ—¥å¿—
        model = torch.nn.Linear(10, 1)
        logger.log_model_info(model, 11, 11)
        
        # æµ‹è¯•é…ç½®æ—¥å¿—
        config = {'test': 'value'}
        logger.log_config(config)
        
        # æµ‹è¯•æ£€æŸ¥ç‚¹æ—¥å¿—
        logger.log_checkpoint_save("/tmp/test.pth", 0, 0.5, True)
        
        # æµ‹è¯•è®­ç»ƒå®Œæˆæ—¥å¿—
        logger.log_training_complete(3600, 0.3)
        
        # æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ
        report = logger.create_training_report()
        assert isinstance(report, dict), "è®­ç»ƒæŠ¥å‘Šåº”è¯¥æ˜¯å­—å…¸"
        
        # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦åˆ›å»º
        log_files = list(Path(temp_dir).glob("*.log"))
        assert len(log_files) > 0, "åº”è¯¥åˆ›å»ºæ—¥å¿—æ–‡ä»¶"
    
    print("âœ“ TrainingLogger æµ‹è¯•é€šè¿‡")

def test_diffusion_trainer_initialization():
    """æµ‹è¯•DiffusionTraineråˆå§‹åŒ–"""
    print("\n=== æµ‹è¯• DiffusionTrainer åˆå§‹åŒ– ===")
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        # åˆ›å»ºé…ç½®
        required_config = {
            'train_lr_dir': '/tmp/train/lr',
            'train_hr_dir': '/tmp/train/hr',
            'val_lr_dir': '/tmp/val/lr',
            'val_hr_dir': '/tmp/val/hr'
        }
        config = ConfigManager(required_config)
        config.update_config({
            'run_dir': temp_dir,
            'device': 'cpu',
            'unet_channels': [32, 64],
            'num_timesteps': 100
        })
        
        # æµ‹è¯•DiffusionTraineråˆå§‹åŒ–
        try:
            trainer = DiffusionTrainer(config)
            
            # æ£€æŸ¥å¿…è¦çš„å±æ€§æ˜¯å¦å­˜åœ¨
            assert hasattr(trainer, 'results_dir'), "åº”è¯¥æœ‰results_dirå±æ€§"
            assert hasattr(trainer, 'logs_dir'), "åº”è¯¥æœ‰logs_dirå±æ€§"
            assert hasattr(trainer, 'checkpoint_dir'), "åº”è¯¥æœ‰checkpoint_dirå±æ€§"
            assert hasattr(trainer, 'config_manager'), "åº”è¯¥æœ‰config_managerå±æ€§"
            assert hasattr(trainer, 'logger'), "åº”è¯¥æœ‰loggerå±æ€§"
            
            # æ£€æŸ¥ç›®å½•æ˜¯å¦åˆ›å»º
            assert os.path.exists(trainer.results_dir), "results_diråº”è¯¥å­˜åœ¨"
            assert os.path.exists(trainer.logs_dir), "logs_diråº”è¯¥å­˜åœ¨"
            assert os.path.exists(trainer.checkpoint_dir), "checkpoint_diråº”è¯¥å­˜åœ¨"
            
            print("âœ“ DiffusionTrainer åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âœ— DiffusionTrainer åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\n=== æµ‹è¯•é”™è¯¯å¤„ç† ===")
    
    # æµ‹è¯•ConfigManageré”™è¯¯å¤„ç†
    required_config = {
        'train_lr_dir': '/tmp/train/lr',
        'train_hr_dir': '/tmp/train/hr',
        'val_lr_dir': '/tmp/val/lr',
        'val_hr_dir': '/tmp/val/hr'
    }
    config = ConfigManager(required_config)
    
    # æµ‹è¯•ç±»å‹é”™è¯¯
    try:
        config.update_config({'batch_size': 'invalid_type'})
        assert False, "åº”è¯¥æŠ›å‡ºç±»å‹é”™è¯¯"
    except (ValueError, TypeError):
        pass  # é¢„æœŸçš„é”™è¯¯
    
    # æµ‹è¯•è´Ÿå€¼é”™è¯¯
    try:
        config.update_config({'batch_size': -5})
        assert False, "åº”è¯¥æ‹’ç»è´Ÿæ•°batch_size"
    except ValueError:
        pass  # é¢„æœŸçš„é”™è¯¯
    
    print("âœ“ é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")

def test_directory_management():
    """æµ‹è¯•ç›®å½•ç®¡ç†"""
    print("\n=== æµ‹è¯•ç›®å½•ç®¡ç† ===")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        required_config = {
            'train_lr_dir': '/tmp/train/lr',
            'train_hr_dir': '/tmp/train/hr',
            'val_lr_dir': '/tmp/val/lr',
            'val_hr_dir': '/tmp/val/hr'
        }
        config = ConfigManager(required_config)
        config.update_config({
            'run_dir': temp_dir,
            'device': 'cpu'
        })
        
        # åˆ›å»ºtrainer
        trainer = DiffusionTrainer(config)
        
        # æ£€æŸ¥æ‰€æœ‰å¿…è¦çš„ç›®å½•æ˜¯å¦åˆ›å»º
        expected_dirs = [
            trainer.run_dir,
            trainer.checkpoint_dir,
            trainer.results_dir,
            trainer.logs_dir,
            trainer.images_dir
        ]
        
        for dir_path in expected_dirs:
            assert os.path.exists(dir_path), f"ç›®å½• {dir_path} åº”è¯¥å­˜åœ¨"
            assert os.path.isdir(dir_path), f"{dir_path} åº”è¯¥æ˜¯ç›®å½•"
        
        print("âœ“ ç›®å½•ç®¡ç†æµ‹è¯•é€šè¿‡")

def test_configuration_inheritance():
    """æµ‹è¯•é…ç½®ç»§æ‰¿"""
    print("\n=== æµ‹è¯•é…ç½®ç»§æ‰¿ ===")
    
    # åˆ›å»ºåŸºç¡€é…ç½®ï¼ˆåŒ…å«å¿…éœ€çš„é…ç½®é¡¹ï¼‰
    base_config = {
        'batch_size': 8,
        'learning_rate': 2e-4,
        'custom_param': 'base_value',
        'train_lr_dir': '/tmp/train/lr',
        'train_hr_dir': '/tmp/train/hr',
        'val_lr_dir': '/tmp/val/lr',
        'val_hr_dir': '/tmp/val/hr'
    }
    
    # åˆ›å»ºç»§æ‰¿é…ç½®
    override_config = {
        'batch_size': 16,  # è¦†ç›–
        'new_param': 'new_value'  # æ–°å¢
    }
    
    config = ConfigManager(base_config)
    config.update_config(override_config)
    
    # æ£€æŸ¥ç»§æ‰¿ç»“æœ
    assert config.get('batch_size') == 16, "batch_sizeåº”è¯¥è¢«è¦†ç›–"
    assert config.get('learning_rate') == 2e-4, "learning_rateåº”è¯¥ä¿æŒä¸å˜"
    assert config.get('custom_param') == 'base_value', "custom_paramåº”è¯¥ä¿æŒä¸å˜"
    assert config.get('new_param') == 'new_value', "new_paramåº”è¯¥è¢«æ·»åŠ "
    
    print("âœ“ é…ç½®ç»§æ‰¿æµ‹è¯•é€šè¿‡")

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹æµ‹è¯•ä»£ç è´¨é‡å’Œå¯ç»´æŠ¤æ€§æ”¹è¿›...")
    
    try:
        test_config_manager()
        test_training_logger()
        test_diffusion_trainer_initialization()
        test_error_handling()
        test_directory_management()
        test_configuration_inheritance()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ”¹è¿›å®æ–½æˆåŠŸã€‚")
        print("\næ”¹è¿›æ€»ç»“:")
        print("1. âœ“ ConfigManager: ç»Ÿä¸€é…ç½®ç®¡ç†ï¼Œæ”¯æŒéªŒè¯å’Œç»§æ‰¿")
        print("2. âœ“ TrainingLogger: ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿï¼Œæ”¯æŒå¤šçº§åˆ«æ—¥å¿—")
        print("3. âœ“ BaseTrainer: å¢å¼ºé”™è¯¯å¤„ç†å’Œç›®å½•ç®¡ç†")
        print("4. âœ“ DiffusionTrainer: ä¿®å¤AttributeErrorï¼Œé›†æˆæ–°ç³»ç»Ÿ")
        print("5. âœ“ é”™è¯¯å¤„ç†: å…¨é¢çš„å¼‚å¸¸æ•è·å’Œæ—¥å¿—è®°å½•")
        print("6. âœ“ ç›®å½•ç®¡ç†: è‡ªåŠ¨åˆ›å»ºå’ŒéªŒè¯å¿…è¦ç›®å½•")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()