#!/usr/bin/env python3
"""
CPUç‰ˆæœ¬çš„ç®€åŒ–æ‰©æ•£æ¨¡å‹è¶…åˆ†è¾¨ç‡è®­ç»ƒè„šæœ¬
é¿å…CUDAå…¼å®¹æ€§é—®é¢˜
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from PIL import Image
import os
import numpy as np
import time
from pathlib import Path

# å¼ºåˆ¶ä½¿ç”¨CPU
def get_device():
    print("ğŸ–¥ï¸ å¼ºåˆ¶ä½¿ç”¨CPUä»¥é¿å…CUDAå…¼å®¹æ€§é—®é¢˜")
    return torch.device('cpu')

class SimpleSRDataset(Dataset):
    """ç®€åŒ–çš„è¶…åˆ†è¾¨ç‡æ•°æ®é›†"""
    def __init__(self, size=20, patch_size=32):
        self.size = size
        self.patch_size = patch_size
        print(f"ğŸ“ åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†: {size} ä¸ªæ ·æœ¬")
        
        # ç®€å•çš„å˜æ¢
        self.transform = transforms.Compose([
            transforms.ToTensor(),
        ])
    
    def __len__(self):
        return self.size
    
    def __getitem__(self, idx):
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„LRå’ŒHRå›¾åƒå¯¹
        # LR: 8x8, HR: 32x32 (4xè¶…åˆ†è¾¨ç‡)
        lr_size = self.patch_size // 4
        hr_size = self.patch_size
        
        # åˆ›å»ºæœ‰ç»“æ„çš„æ¨¡æ‹Ÿæ•°æ®è€Œä¸æ˜¯çº¯éšæœº
        base_pattern = torch.randn(3, lr_size, lr_size)
        lr_tensor = torch.clamp(base_pattern, 0, 1)
        
        # HRå›¾åƒæ˜¯LRçš„ä¸Šé‡‡æ ·ç‰ˆæœ¬åŠ ä¸Šä¸€äº›ç»†èŠ‚
        hr_tensor = nn.functional.interpolate(
            lr_tensor.unsqueeze(0), 
            size=(hr_size, hr_size), 
            mode='bilinear', 
            align_corners=False
        ).squeeze(0)
        
        # æ·»åŠ ä¸€äº›é«˜é¢‘ç»†èŠ‚
        detail = torch.randn(3, hr_size, hr_size) * 0.1
        hr_tensor = torch.clamp(hr_tensor + detail, 0, 1)
        
        return lr_tensor, hr_tensor

class SimpleUNet(nn.Module):
    """æç®€çš„U-Netæ¨¡å‹ - CPUä¼˜åŒ–ç‰ˆæœ¬"""
    def __init__(self, in_channels=3, out_channels=3):
        super().__init__()
        
        # æ›´å°çš„ç½‘ç»œä»¥é€‚åº”CPUè®­ç»ƒ
        self.enc1 = nn.Sequential(
            nn.Conv2d(in_channels, 16, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(16, 16, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        
        self.enc2 = nn.Sequential(
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 32, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        
        # è§£ç å™¨
        self.dec2 = nn.Sequential(
            nn.ConvTranspose2d(32, 16, 2, stride=2),
            nn.ReLU(inplace=True)
        )
        
        self.dec1 = nn.Sequential(
            nn.Conv2d(32, 16, 3, padding=1),  # 32 = 16 + 16 (skip connection)
            nn.ReLU(inplace=True),
            nn.Conv2d(16, out_channels, 3, padding=1),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # ä¸Šé‡‡æ ·è¾“å…¥
        x = nn.functional.interpolate(x, scale_factor=4, mode='bilinear', align_corners=False)
        
        # ç¼–ç 
        e1 = self.enc1(x)
        e2 = self.enc2(e1)
        
        # è§£ç 
        d2 = self.dec2(e2)
        d1 = self.dec1(torch.cat([d2, e1], dim=1))
        
        return d1

def add_simple_noise(image, noise_level=0.1):
    """æ·»åŠ ç®€å•çš„é«˜æ–¯å™ªå£°"""
    noise = torch.randn_like(image) * noise_level
    return torch.clamp(image + noise, 0, 1), noise

def train_epoch(model, dataloader, optimizer, criterion, device, epoch):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    num_batches = len(dataloader)
    
    print(f"Epoch {epoch} å¼€å§‹è®­ç»ƒ...")
    
    for batch_idx, (lr_images, hr_images) in enumerate(dataloader):
        try:
            lr_images = lr_images.to(device)
            hr_images = hr_images.to(device)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            predicted = model(lr_images)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(predicted, hr_images)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # æ‰“å°è¿›åº¦
            if batch_idx % 5 == 0:
                print(f"  Batch {batch_idx}/{num_batches}, Loss: {loss.item():.6f}")
                
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡ {batch_idx} è®­ç»ƒå¤±è´¥: {e}")
            continue
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0
    print(f"Epoch {epoch} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.6f}")
    return avg_loss

def main():
    print("ğŸš€ å¼€å§‹CPUç‰ˆæœ¬ç®€åŒ–æ‰©æ•£æ¨¡å‹è®­ç»ƒ")
    print("=" * 50)
    
    # å¼ºåˆ¶ä½¿ç”¨CPU
    device = get_device()
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # è®­ç»ƒé…ç½® - CPUä¼˜åŒ–
    config = {
        'epochs': 3,
        'batch_size': 2,
        'learning_rate': 1e-3,  # ç¨å¾®æé«˜å­¦ä¹ ç‡ä»¥åŠ å¿«æ”¶æ•›
        'patch_size': 32,       # æ›´å°çš„patchå¤§å°
        'num_samples': 20       # æ›´å°‘çš„æ ·æœ¬
    }
    
    print("è®­ç»ƒé…ç½®:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†
    train_dataset = SimpleSRDataset(
        size=config['num_samples'],
        patch_size=config['patch_size']
    )
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True,
        num_workers=0  # CPUè®­ç»ƒä¸éœ€è¦å¤šè¿›ç¨‹
    )
    
    print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    
    # åˆ›å»ºæ¨¡å‹
    print("\nğŸ§  åˆ›å»ºæ¨¡å‹...")
    model = SimpleUNet().to(device)
    
    # è®¡ç®—å‚æ•°æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])
    criterion = nn.MSELoss()
    
    print(f"ä¼˜åŒ–å™¨: Adam")
    print(f"æŸå¤±å‡½æ•°: MSE Loss")
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ ({config['epochs']} epochs)...")
    print("=" * 50)
    
    training_history = []
    
    try:
        for epoch in range(1, config['epochs'] + 1):
            start_time = time.time()
            
            # è®­ç»ƒä¸€ä¸ªepoch
            train_loss = train_epoch(model, train_loader, optimizer, criterion, device, epoch)
            
            epoch_time = time.time() - start_time
            training_history.append(train_loss)
            
            print(f"Epoch {epoch} å®Œæˆï¼Œç”¨æ—¶: {epoch_time:.2f}ç§’")
            print("-" * 30)
            
        print("\nâœ… è®­ç»ƒå®Œæˆ!")
        print(f"æœ€ç»ˆæŸå¤±: {training_history[-1]:.6f}")
        
        # ä¿å­˜æ¨¡å‹
        model_path = "simple_diffusion_cpu_model.pth"
        torch.save({
            'model_state_dict': model.state_dict(),
            'config': config,
            'training_history': training_history
        }, model_path)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        # ç®€å•æµ‹è¯•
        print("\nğŸ§ª è¿›è¡Œç®€å•æµ‹è¯•...")
        model.eval()
        with torch.no_grad():
            test_lr = torch.randn(1, 3, 8, 8).to(device)
            test_output = model(test_lr)
            print(f"æµ‹è¯•è¾“å…¥å½¢çŠ¶: {test_lr.shape}")
            print(f"æµ‹è¯•è¾“å‡ºå½¢çŠ¶: {test_output.shape}")
            print("âœ… æ¨¡å‹æµ‹è¯•é€šè¿‡!")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    return model, training_history

if __name__ == "__main__":
    model, history = main()